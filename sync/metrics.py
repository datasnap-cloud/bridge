"""
M√≥dulo para coleta e gerenciamento de m√©tricas do pipeline de sincroniza√ß√£o.
Coleta m√©tricas de performance, erros e estat√≠sticas de uso.
"""

import json
import logging
import time
import psutil
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
from threading import Lock
import statistics

from core.timeutil import get_current_timestamp, format_duration
from core.paths import BridgePaths


logger = logging.getLogger(__name__)


@dataclass
class SyncMetrics:
    """M√©tricas de uma sincroniza√ß√£o."""
    mapping_name: str
    schema_slug: str
    start_time: int
    end_time: Optional[int] = None
    duration: Optional[int] = None
    
    # M√©tricas de dados
    records_extracted: int = 0
    records_written: int = 0
    records_uploaded: int = 0
    records_deleted: int = 0
    
    # M√©tricas de arquivos
    files_created: int = 0
    files_uploaded: int = 0
    total_file_size: int = 0
    
    # M√©tricas de performance
    extraction_time: Optional[int] = None
    writing_time: Optional[int] = None
    upload_time: Optional[int] = None
    
    # M√©tricas de sistema
    peak_memory_mb: Optional[float] = None
    avg_cpu_percent: Optional[float] = None
    
    # Status e erros
    success: bool = False
    error_message: Optional[str] = None
    retry_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio."""
        return asdict(self)
    
    def calculate_rates(self) -> Dict[str, float]:
        """Calcula taxas de processamento."""
        if not self.duration or self.duration == 0:
            return {}
        
        return {
            'records_per_second': self.records_extracted / self.duration,
            'mb_per_second': (self.total_file_size / (1024 * 1024)) / self.duration,
            'files_per_minute': (self.files_created / self.duration) * 60
        }


@dataclass
class SystemMetrics:
    """M√©tricas do sistema."""
    timestamp: int
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_usage_percent: float
    disk_free_gb: float
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio."""
        return asdict(self)


class MetricsCollector:
    """Coletor de m√©tricas em tempo real."""
    
    def __init__(self, collection_interval: int = 5):
        """
        Inicializa o coletor.
        
        Args:
            collection_interval: Intervalo de coleta em segundos
        """
        self.collection_interval = collection_interval
        self.system_metrics: deque = deque(maxlen=1000)  # √öltimas 1000 amostras
        self.sync_metrics: List[SyncMetrics] = []
        self.current_sync: Optional[SyncMetrics] = None
        
        self._lock = Lock()
        self._collecting = False
        
        # M√©tricas agregadas
        self.daily_stats = defaultdict(lambda: {
            'syncs_count': 0,
            'total_records': 0,
            'total_files': 0,
            'total_size': 0,
            'success_count': 0,
            'error_count': 0
        })
    
    def start_sync_metrics(self, mapping_name: str, schema_slug: str) -> SyncMetrics:
        """
        Inicia coleta de m√©tricas para uma sincroniza√ß√£o.
        
        Args:
            mapping_name: Nome do mapeamento
            schema_slug: Slug do schema
            
        Returns:
            Inst√¢ncia de m√©tricas da sincroniza√ß√£o
        """
        with self._lock:
            self.current_sync = SyncMetrics(
                mapping_name=mapping_name,
                schema_slug=schema_slug,
                start_time=get_current_timestamp()
            )
            
            logger.debug(f"Iniciando coleta de m√©tricas: {mapping_name}")
            return self.current_sync
    
    def finish_sync_metrics(self, success: bool = True, error_message: str = None) -> Optional[SyncMetrics]:
        """
        Finaliza coleta de m√©tricas para uma sincroniza√ß√£o.
        
        Args:
            success: Se a sincroniza√ß√£o foi bem-sucedida
            error_message: Mensagem de erro (se houver)
            
        Returns:
            M√©tricas finalizadas ou None
        """
        with self._lock:
            if not self.current_sync:
                return None
            
            # Finaliza m√©tricas
            self.current_sync.end_time = get_current_timestamp()
            self.current_sync.duration = self.current_sync.end_time - self.current_sync.start_time
            self.current_sync.success = success
            self.current_sync.error_message = error_message
            
            # Calcula m√©tricas de sistema
            if self.system_metrics:
                recent_metrics = list(self.system_metrics)[-10:]  # √öltimas 10 amostras
                self.current_sync.peak_memory_mb = max(m.memory_used_mb for m in recent_metrics)
                self.current_sync.avg_cpu_percent = statistics.mean(m.cpu_percent for m in recent_metrics)
            
            # Adiciona √†s m√©tricas coletadas
            completed_sync = self.current_sync
            self.sync_metrics.append(completed_sync)
            
            # Atualiza estat√≠sticas di√°rias
            self._update_daily_stats(completed_sync)
            
            self.current_sync = None
            
            logger.debug(f"M√©tricas finalizadas: {completed_sync.mapping_name}")
            return completed_sync
    
    def update_extraction_metrics(self, records_count: int, duration: int) -> None:
        """
        Atualiza m√©tricas de extra√ß√£o.
        
        Args:
            records_count: N√∫mero de registros extra√≠dos
            duration: Dura√ß√£o da extra√ß√£o
        """
        with self._lock:
            if self.current_sync:
                self.current_sync.records_extracted = records_count
                self.current_sync.extraction_time = duration
    
    def update_writing_metrics(self, records_count: int, files_count: int, 
                              total_size: int, duration: int) -> None:
        """
        Atualiza m√©tricas de escrita.
        
        Args:
            records_count: N√∫mero de registros escritos
            files_count: N√∫mero de arquivos criados
            total_size: Tamanho total dos arquivos
            duration: Dura√ß√£o da escrita
        """
        with self._lock:
            if self.current_sync:
                self.current_sync.records_written = records_count
                self.current_sync.files_created = files_count
                self.current_sync.total_file_size = total_size
                self.current_sync.writing_time = duration
    
    def update_upload_metrics(self, files_uploaded: int, records_uploaded: int, 
                             duration: int, retry_count: int = 0) -> None:
        """
        Atualiza m√©tricas de upload.
        
        Args:
            files_uploaded: N√∫mero de arquivos enviados
            records_uploaded: N√∫mero de registros enviados
            duration: Dura√ß√£o do upload
            retry_count: N√∫mero de tentativas
        """
        with self._lock:
            if self.current_sync:
                self.current_sync.files_uploaded = files_uploaded
                self.current_sync.records_uploaded = records_uploaded
                self.current_sync.upload_time = duration
                self.current_sync.retry_count = retry_count
    
    def update_deletion_metrics(self, records_deleted: int, mapping_name: str) -> None:
        """
        Atualiza m√©tricas de dele√ß√£o ap√≥s upload.
        
        Args:
            records_deleted: N√∫mero de registros deletados
            mapping_name: Nome do mapeamento
        """
        with self._lock:
            if self.current_sync:
                # Adiciona informa√ß√£o de dele√ß√£o √†s m√©tricas atuais
                if not hasattr(self.current_sync, 'records_deleted'):
                    self.current_sync.records_deleted = 0
                self.current_sync.records_deleted = records_deleted
                
                logger.info(f"üìä M√©tricas de dele√ß√£o atualizadas: {records_deleted} registros deletados para {mapping_name}")
    
    def collect_system_metrics(self) -> SystemMetrics:
        """
        Coleta m√©tricas do sistema.
        
        Returns:
            M√©tricas do sistema atual
        """
        try:
            # CPU e mem√≥ria
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            # Disco (diret√≥rio do projeto)
            paths = BridgePaths()
            disk_usage = psutil.disk_usage(str(paths.get_app_root()))
            
            metrics = SystemMetrics(
                timestamp=get_current_timestamp(),
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_mb=memory.used / (1024 * 1024),
                disk_usage_percent=(disk_usage.used / disk_usage.total) * 100,
                disk_free_gb=disk_usage.free / (1024 * 1024 * 1024)
            )
            
            with self._lock:
                self.system_metrics.append(metrics)
            
            return metrics
            
        except Exception as e:
            logger.warning(f"Erro ao coletar m√©tricas do sistema: {e}")
            return SystemMetrics(
                timestamp=get_current_timestamp(),
                cpu_percent=0,
                memory_percent=0,
                memory_used_mb=0,
                disk_usage_percent=0,
                disk_free_gb=0
            )
    
    def _update_daily_stats(self, sync_metrics: SyncMetrics) -> None:
        """
        Atualiza estat√≠sticas di√°rias.
        
        Args:
            sync_metrics: M√©tricas da sincroniza√ß√£o
        """
        # Usa data como chave (YYYY-MM-DD)
        date_key = time.strftime('%Y-%m-%d', time.localtime(sync_metrics.start_time))
        
        stats = self.daily_stats[date_key]
        stats['syncs_count'] += 1
        stats['total_records'] += sync_metrics.records_extracted
        stats['total_files'] += sync_metrics.files_created
        stats['total_size'] += sync_metrics.total_file_size
        
        if sync_metrics.success:
            stats['success_count'] += 1
        else:
            stats['error_count'] += 1
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """
        Obt√©m m√©tricas atuais.
        
        Returns:
            M√©tricas atuais do sistema e sincroniza√ß√£o
        """
        with self._lock:
            result = {
                'system': None,
                'current_sync': None,
                'recent_syncs': []
            }
            
            # M√©tricas do sistema
            if self.system_metrics:
                result['system'] = self.system_metrics[-1].to_dict()
            
            # Sincroniza√ß√£o atual
            if self.current_sync:
                current_dict = self.current_sync.to_dict()
                if self.current_sync.start_time:
                    current_dict['elapsed_time'] = get_current_timestamp() - self.current_sync.start_time
                result['current_sync'] = current_dict
            
            # Sincroniza√ß√µes recentes
            result['recent_syncs'] = [s.to_dict() for s in self.sync_metrics[-10:]]
            
            return result
    
    def get_performance_summary(self, hours: int = 24) -> Dict[str, Any]:
        """
        Obt√©m resumo de performance.
        
        Args:
            hours: N√∫mero de horas para an√°lise
            
        Returns:
            Resumo de performance
        """
        cutoff_time = get_current_timestamp() - (hours * 3600)
        
        with self._lock:
            # Filtra sincroniza√ß√µes recentes
            recent_syncs = [
                s for s in self.sync_metrics 
                if s.start_time >= cutoff_time
            ]
            
            if not recent_syncs:
                return {
                    'period_hours': hours,
                    'total_syncs': 0,
                    'success_rate': 0,
                    'avg_duration': 0,
                    'total_records': 0,
                    'total_files': 0,
                    'total_size_mb': 0
                }
            
            # Calcula estat√≠sticas
            successful_syncs = [s for s in recent_syncs if s.success]
            
            total_records = sum(s.records_extracted for s in recent_syncs)
            total_files = sum(s.files_created for s in recent_syncs)
            total_size = sum(s.total_file_size for s in recent_syncs)
            
            durations = [s.duration for s in recent_syncs if s.duration]
            avg_duration = statistics.mean(durations) if durations else 0
            
            return {
                'period_hours': hours,
                'total_syncs': len(recent_syncs),
                'successful_syncs': len(successful_syncs),
                'success_rate': (len(successful_syncs) / len(recent_syncs)) * 100,
                'avg_duration': avg_duration,
                'avg_duration_formatted': format_duration(int(avg_duration)),
                'total_records': total_records,
                'total_files': total_files,
                'total_size_mb': total_size / (1024 * 1024),
                'avg_records_per_sync': total_records / len(recent_syncs),
                'avg_files_per_sync': total_files / len(recent_syncs)
            }
    
    def get_error_analysis(self, hours: int = 24) -> Dict[str, Any]:
        """
        Analisa erros recentes.
        
        Args:
            hours: N√∫mero de horas para an√°lise
            
        Returns:
            An√°lise de erros
        """
        cutoff_time = get_current_timestamp() - (hours * 3600)
        
        with self._lock:
            # Filtra sincroniza√ß√µes com erro
            error_syncs = [
                s for s in self.sync_metrics 
                if s.start_time >= cutoff_time and not s.success
            ]
            
            if not error_syncs:
                return {
                    'period_hours': hours,
                    'total_errors': 0,
                    'error_types': {},
                    'affected_mappings': [],
                    'recent_errors': []
                }
            
            # Agrupa erros por tipo
            error_types = defaultdict(int)
            affected_mappings = set()
            
            for sync in error_syncs:
                if sync.error_message:
                    # Simplifica mensagem de erro para agrupamento
                    error_key = sync.error_message.split(':')[0] if ':' in sync.error_message else sync.error_message
                    error_types[error_key] += 1
                
                affected_mappings.add(sync.mapping_name)
            
            return {
                'period_hours': hours,
                'total_errors': len(error_syncs),
                'error_types': dict(error_types),
                'affected_mappings': list(affected_mappings),
                'recent_errors': [
                    {
                        'mapping_name': s.mapping_name,
                        'error_message': s.error_message,
                        'timestamp': s.start_time,
                        'retry_count': s.retry_count
                    }
                    for s in error_syncs[-10:]  # √öltimos 10 erros
                ]
            }


class MetricsStorage:
    """Armazenamento persistente de m√©tricas."""
    
    def __init__(self, storage_dir: Path = None):
        """
        Inicializa o armazenamento.
        
        Args:
            storage_dir: Diret√≥rio de armazenamento
        """
        self.paths = BridgePaths()
        self.storage_dir = storage_dir or self.paths.get_logs_dir()
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.metrics_file = self.storage_dir / "sync_metrics.jsonl"
        self.daily_stats_file = self.storage_dir / "daily_stats.json"
    
    def save_sync_metrics(self, metrics: SyncMetrics) -> None:
        """
        Salva m√©tricas de sincroniza√ß√£o.
        
        Args:
            metrics: M√©tricas a serem salvas
        """
        try:
            with open(self.metrics_file, 'a', encoding='utf-8') as f:
                json.dump(metrics.to_dict(), f, ensure_ascii=False)
                f.write('\n')
            
            logger.debug(f"M√©tricas salvas: {metrics.mapping_name}")
            
        except Exception as e:
            logger.error(f"Erro ao salvar m√©tricas: {e}")
    
    def save_daily_stats(self, daily_stats: Dict[str, Dict[str, Any]]) -> None:
        """
        Salva estat√≠sticas di√°rias.
        
        Args:
            daily_stats: Estat√≠sticas di√°rias
        """
        try:
            with open(self.daily_stats_file, 'w', encoding='utf-8') as f:
                json.dump(daily_stats, f, ensure_ascii=False, indent=2)
            
            logger.debug("Estat√≠sticas di√°rias salvas")
            
        except Exception as e:
            logger.error(f"Erro ao salvar estat√≠sticas di√°rias: {e}")
    
    def load_sync_metrics(self, days: int = 30) -> List[SyncMetrics]:
        """
        Carrega m√©tricas de sincroniza√ß√£o.
        
        Args:
            days: N√∫mero de dias para carregar
            
        Returns:
            Lista de m√©tricas
        """
        metrics = []
        cutoff_time = get_current_timestamp() - (days * 24 * 3600)
        
        try:
            if not self.metrics_file.exists():
                return metrics
            
            with open(self.metrics_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        data = json.loads(line)
                        
                        # Filtra por data
                        if data.get('start_time', 0) >= cutoff_time:
                            # Converte de volta para SyncMetrics
                            sync_metrics = SyncMetrics(**data)
                            metrics.append(sync_metrics)
                            
                    except json.JSONDecodeError:
                        continue
            
            logger.debug(f"Carregadas {len(metrics)} m√©tricas dos √∫ltimos {days} dias")
            
        except Exception as e:
            logger.error(f"Erro ao carregar m√©tricas: {e}")
        
        return metrics
    
    def load_daily_stats(self) -> Dict[str, Dict[str, Any]]:
        """
        Carrega estat√≠sticas di√°rias.
        
        Returns:
            Estat√≠sticas di√°rias
        """
        try:
            if not self.daily_stats_file.exists():
                return {}
            
            with open(self.daily_stats_file, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            logger.error(f"Erro ao carregar estat√≠sticas di√°rias: {e}")
            return {}
    
    def cleanup_old_metrics(self, days: int = 90) -> None:
        """
        Remove m√©tricas antigas.
        
        Args:
            days: N√∫mero de dias para manter
        """
        try:
            if not self.metrics_file.exists():
                return
            
            cutoff_time = get_current_timestamp() - (days * 24 * 3600)
            temp_file = self.metrics_file.with_suffix('.tmp')
            
            kept_count = 0
            removed_count = 0
            
            with open(self.metrics_file, 'r', encoding='utf-8') as input_f, \
                 open(temp_file, 'w', encoding='utf-8') as output_f:
                
                for line in input_f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        data = json.loads(line)
                        
                        if data.get('start_time', 0) >= cutoff_time:
                            output_f.write(line + '\n')
                            kept_count += 1
                        else:
                            removed_count += 1
                            
                    except json.JSONDecodeError:
                        # Mant√©m linhas inv√°lidas
                        output_f.write(line + '\n')
            
            # Substitui arquivo original
            temp_file.replace(self.metrics_file)
            
            logger.info(f"Limpeza de m√©tricas: {kept_count} mantidas, {removed_count} removidas")
            
        except Exception as e:
            logger.error(f"Erro na limpeza de m√©tricas: {e}")


# Inst√¢ncia global do coletor
_metrics_collector = None
_metrics_storage = None


def get_metrics_collector() -> MetricsCollector:
    """
    Obt√©m inst√¢ncia global do coletor de m√©tricas.
    
    Returns:
        Inst√¢ncia do coletor
    """
    global _metrics_collector
    if _metrics_collector is None:
        _metrics_collector = MetricsCollector()
    return _metrics_collector


def get_metrics_storage() -> MetricsStorage:
    """
    Obt√©m inst√¢ncia global do armazenamento de m√©tricas.
    
    Returns:
        Inst√¢ncia do armazenamento
    """
    global _metrics_storage
    if _metrics_storage is None:
        _metrics_storage = MetricsStorage()
    return _metrics_storage


def save_metrics_on_exit(collector: MetricsCollector, storage: MetricsStorage) -> None:
    """
    Salva m√©tricas ao sair da aplica√ß√£o.
    
    Args:
        collector: Coletor de m√©tricas
        storage: Armazenamento de m√©tricas
    """
    try:
        # Salva m√©tricas pendentes
        for sync_metrics in collector.sync_metrics:
            storage.save_sync_metrics(sync_metrics)
        
        # Salva estat√≠sticas di√°rias
        storage.save_daily_stats(dict(collector.daily_stats))
        
        logger.info("M√©tricas salvas ao sair da aplica√ß√£o")
        
    except Exception as e:
        logger.error(f"Erro ao salvar m√©tricas na sa√≠da: {e}")


def format_metrics_report(metrics: Dict[str, Any]) -> str:
    """
    Formata relat√≥rio de m√©tricas para exibi√ß√£o.
    
    Args:
        metrics: Dados de m√©tricas
        
    Returns:
        Relat√≥rio formatado
    """
    lines = []
    
    # Cabe√ßalho
    lines.append("=== RELAT√ìRIO DE M√âTRICAS ===")
    lines.append("")
    
    # Sistema atual
    if metrics.get('system'):
        sys = metrics['system']
        lines.append("Sistema:")
        lines.append(f"  CPU: {sys['cpu_percent']:.1f}%")
        lines.append(f"  Mem√≥ria: {sys['memory_percent']:.1f}% ({sys['memory_used_mb']:.0f} MB)")
        lines.append(f"  Disco: {sys['disk_usage_percent']:.1f}% ({sys['disk_free_gb']:.1f} GB livres)")
        lines.append("")
    
    # Sincroniza√ß√£o atual
    if metrics.get('current_sync'):
        sync = metrics['current_sync']
        lines.append("Sincroniza√ß√£o Atual:")
        lines.append(f"  Mapeamento: {sync['mapping_name']}")
        lines.append(f"  Schema: {sync['schema_slug']}")
        if sync.get('elapsed_time'):
            lines.append(f"  Tempo decorrido: {format_duration(sync['elapsed_time'])}")
        lines.append(f"  Registros extra√≠dos: {sync['records_extracted']}")
        lines.append("")
    
    # Sincroniza√ß√µes recentes
    if metrics.get('recent_syncs'):
        lines.append("Sincroniza√ß√µes Recentes:")
        for sync in metrics['recent_syncs'][-5:]:  # √öltimas 5
            status = "‚úì" if sync['success'] else "‚úó"
            duration = format_duration(sync['duration']) if sync['duration'] else "N/A"
            lines.append(f"  {status} {sync['mapping_name']} - {sync['records_extracted']} registros ({duration})")
        lines.append("")
    
    return "\n".join(lines)